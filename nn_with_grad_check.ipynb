{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('xor.dat', header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1  2\n",
       "0  0.0  0.0  0\n",
       "1  1.0  0.0  1\n",
       "2  0.0  1.0  1\n",
       "3  1.0  1.0  0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parameters(dataset):\n",
    "    train = dataset.sample(frac=1)\n",
    "    X = train.drop(2, axis=1)\n",
    "    Y = train[2]\n",
    "\n",
    "    # One hot encoding for Y\n",
    "    Y = pd.get_dummies(Y)\n",
    "\n",
    "    num_classes = Y.shape[1]\n",
    "    num_features = X.shape[1]\n",
    "    N = X.shape[0]\n",
    "\n",
    "    # Weights \n",
    "    W = np.random.rand(num_features, num_classes)\n",
    "    b = np.zeros(num_classes)\n",
    "    \n",
    "    return X, Y, W, b, N, num_classes, num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(logits):\n",
    "    '''\n",
    "        Computes the softmax function value\n",
    "    '''\n",
    "    activation = []\n",
    "    for index in range(0, len(logits)):\n",
    "        exp_sum = 0\n",
    "        for val in logits[index]:\n",
    "            exp_sum += np.exp(val)\n",
    "        activation.append([np.exp(value) / exp_sum for value in logits[index]])\n",
    "        \n",
    "    return activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 2)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_activation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pre_act(input_x, weights, bias):\n",
    "    '''\n",
    "        Compute pre activation i.e. f = X*W + b\n",
    "    '''\n",
    "    \n",
    "    return np.dot(input_x, weights) + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.2988471024610757, 0.7011528975389243],\n",
       " [0.3819850083703233, 0.6180149916296768],\n",
       " [0.40813935339684804, 0.5918606466031521],\n",
       " [0.5, 0.5]]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#XT = np.transpose(X)\n",
    "X, Y, W, b, N, num_classes, num_weights = get_parameters(dataset)\n",
    "\n",
    "# Compute activation in forward propogation\n",
    "pre_activation = compute_pre_act(X, W, b)\n",
    "\n",
    "p = softmax(pre_activation)\n",
    "\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.701153</td>\n",
       "      <td>0.701153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.381985</td>\n",
       "      <td>-0.381985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.408139</td>\n",
       "      <td>-0.408139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1\n",
       "3 -0.701153  0.701153\n",
       "2  0.381985 -0.381985\n",
       "1  0.408139 -0.408139\n",
       "0 -0.500000  0.500000"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gradient of loss wrt pre activation Wx + b.\n",
    "grad_loss = np.subtract(p, Y)\n",
    "grad_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.07265623,  0.07422221],\n",
       "       [-0.07950053,  0.08056455]])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gradient for weights = XT * (1/N)grad_loss + learn_rate * W\n",
    "XT = np.transpose(X)\n",
    "learn_rate = 0.001\n",
    "\n",
    "W_grad = np.dot(XT, np.divide(grad_loss, N)) + np.multiply(W, learn_rate)\n",
    "\n",
    "W_grad\n",
    "\n",
    "# Gradient for bias = 1/N * summation ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(post_act, num_instances, learning_rate, Weights, Y_values):\n",
    "    '''\n",
    "        Compute loss = -1/N * (summation(log(post_act[i,Yi]))) + (learning_rate / 2) * (summation((Weights[d, k]) ^ 2)\n",
    "    '''\n",
    "    # First part without constant\n",
    "    log_summation = 0\n",
    "    for i in range(0, len(post_act)):\n",
    "        log_summation += np.log(post_act[Y_values[i]])\n",
    "        \n",
    "    # Second part without constant    \n",
    "    weights_summation = 0\n",
    "    for d in range(0, len(Weights)):\n",
    "        for k in range(0, len(Weights[0])):\n",
    "            weights_summation += np.square(Weights[d][k])\n",
    "            \n",
    "    loss = log_summation * (-1/num_instances) + (learning_rate/2) * weights_summation\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_checking(analytical_grad):\n",
    "    '''\n",
    "        Compute the numerical gradient for each weight and check if difference is < 1e-4 for all.\n",
    "    '''\n",
    "    epsilon = 0.001\n",
    "    \n",
    "    log_p = np.multiply(Y, np.log(p))\n",
    "    \n",
    "    # For each row / data point, get loss with weight + epsilon and weight - epsilon\n",
    "    for y_ind in range(0, len(log_p.shape[1])):\n",
    "        L_plus = compute_loss(p[y_ind], N, learn_rate, np.add(W, epsilon))\n",
    "        L_minus = compute_loss(p[y_ind], N, learn_rate, np.subtract(W, epsilon), Y.iloc[y_ind])\n",
    "    \n",
    "        numerical_grad = 0\n",
    "    \n",
    "    if np.abs(numerical_grad - analytical_grad) < (1 * np.power(10, -4)):\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.59615905, 0.9678216 ],\n",
       "       [0.29044213, 0.77157348]])"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.subtract(W, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3917016366703075\n",
      "0.36189263119864157\n",
      "0.356146180384105\n",
      "0.34756210153667344\n"
     ]
    }
   ],
   "source": [
    "for y_ind in range(0, len(Y)):\n",
    "    print(compute_loss(p[y_ind], N, learn_rate, W, Y.iloc[y_ind]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    1\n",
       "2    0\n",
       "1    0\n",
       "0    1\n",
       "Name: 0, dtype: uint8"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1b\n",
    "dataset_spiral = pd.read_csv('spiral_train.dat', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y, W, b, N, num_classes, num_weights = get_parameters(dataset_spiral)\n",
    "\n",
    "# Compute activation in forward propogation\n",
    "pre_activation = compute_pre_act(X, W, b)\n",
    "\n",
    "p = softmax(pre_activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-0.676437</td>\n",
       "      <td>0.326065</td>\n",
       "      <td>0.350373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>0.367151</td>\n",
       "      <td>0.361796</td>\n",
       "      <td>-0.728947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>-0.617728</td>\n",
       "      <td>0.399298</td>\n",
       "      <td>0.218430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>0.296441</td>\n",
       "      <td>-0.723896</td>\n",
       "      <td>0.427454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.678117</td>\n",
       "      <td>0.319030</td>\n",
       "      <td>0.359087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.669465</td>\n",
       "      <td>0.328544</td>\n",
       "      <td>0.340921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>0.352386</td>\n",
       "      <td>0.366542</td>\n",
       "      <td>-0.718928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>0.347930</td>\n",
       "      <td>0.324548</td>\n",
       "      <td>-0.672478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>0.293485</td>\n",
       "      <td>0.257899</td>\n",
       "      <td>-0.551384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>0.276691</td>\n",
       "      <td>-0.736109</td>\n",
       "      <td>0.459417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>0.355830</td>\n",
       "      <td>0.342016</td>\n",
       "      <td>-0.697846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>0.295189</td>\n",
       "      <td>-0.725142</td>\n",
       "      <td>0.429953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>-0.670353</td>\n",
       "      <td>0.346545</td>\n",
       "      <td>0.323808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>0.341604</td>\n",
       "      <td>0.352069</td>\n",
       "      <td>-0.693673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>0.354285</td>\n",
       "      <td>0.369268</td>\n",
       "      <td>-0.723553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>-0.657277</td>\n",
       "      <td>0.369434</td>\n",
       "      <td>0.287843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>-0.620876</td>\n",
       "      <td>0.404349</td>\n",
       "      <td>0.216527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>0.295567</td>\n",
       "      <td>-0.721834</td>\n",
       "      <td>0.426268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>-0.666667</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>0.333274</td>\n",
       "      <td>0.336068</td>\n",
       "      <td>-0.669342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>0.289533</td>\n",
       "      <td>-0.726322</td>\n",
       "      <td>0.436789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.286244</td>\n",
       "      <td>-0.725192</td>\n",
       "      <td>0.438948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>0.311371</td>\n",
       "      <td>0.278660</td>\n",
       "      <td>-0.590031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>0.285675</td>\n",
       "      <td>-0.712078</td>\n",
       "      <td>0.426404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>0.371739</td>\n",
       "      <td>0.381675</td>\n",
       "      <td>-0.753414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>0.349993</td>\n",
       "      <td>0.364316</td>\n",
       "      <td>-0.714310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>0.335113</td>\n",
       "      <td>-0.675201</td>\n",
       "      <td>0.340087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>0.337782</td>\n",
       "      <td>-0.665087</td>\n",
       "      <td>0.327305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>0.363460</td>\n",
       "      <td>0.369751</td>\n",
       "      <td>-0.733210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>-0.638994</td>\n",
       "      <td>0.397917</td>\n",
       "      <td>0.241078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>0.281044</td>\n",
       "      <td>-0.721658</td>\n",
       "      <td>0.440614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.670536</td>\n",
       "      <td>0.327712</td>\n",
       "      <td>0.342825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>0.341168</td>\n",
       "      <td>0.349230</td>\n",
       "      <td>-0.690398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>-0.681543</td>\n",
       "      <td>0.319292</td>\n",
       "      <td>0.362251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>0.366428</td>\n",
       "      <td>0.373407</td>\n",
       "      <td>-0.739835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>0.312488</td>\n",
       "      <td>-0.706968</td>\n",
       "      <td>0.394480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>0.339394</td>\n",
       "      <td>-0.663105</td>\n",
       "      <td>0.323711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>0.302641</td>\n",
       "      <td>-0.717316</td>\n",
       "      <td>0.414675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>-0.675219</td>\n",
       "      <td>0.333249</td>\n",
       "      <td>0.341970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>0.300231</td>\n",
       "      <td>-0.716758</td>\n",
       "      <td>0.416527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>-0.660779</td>\n",
       "      <td>0.364042</td>\n",
       "      <td>0.296737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>0.281085</td>\n",
       "      <td>-0.734549</td>\n",
       "      <td>0.453464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>-0.664794</td>\n",
       "      <td>0.355792</td>\n",
       "      <td>0.309002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>0.329059</td>\n",
       "      <td>0.298012</td>\n",
       "      <td>-0.627072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>0.339573</td>\n",
       "      <td>-0.665631</td>\n",
       "      <td>0.326058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>0.331291</td>\n",
       "      <td>0.300113</td>\n",
       "      <td>-0.631405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>0.315821</td>\n",
       "      <td>-0.701497</td>\n",
       "      <td>0.385676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.671739</td>\n",
       "      <td>0.325630</td>\n",
       "      <td>0.346109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>0.332901</td>\n",
       "      <td>-0.676883</td>\n",
       "      <td>0.343982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>0.337582</td>\n",
       "      <td>-0.669156</td>\n",
       "      <td>0.331574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>0.342472</td>\n",
       "      <td>0.356145</td>\n",
       "      <td>-0.698618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>0.324742</td>\n",
       "      <td>0.295321</td>\n",
       "      <td>-0.620063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>0.343298</td>\n",
       "      <td>0.355195</td>\n",
       "      <td>-0.698493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>-0.675738</td>\n",
       "      <td>0.332017</td>\n",
       "      <td>0.343721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>0.280193</td>\n",
       "      <td>-0.736334</td>\n",
       "      <td>0.456141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>0.335619</td>\n",
       "      <td>0.309321</td>\n",
       "      <td>-0.644940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>0.365148</td>\n",
       "      <td>0.367154</td>\n",
       "      <td>-0.732302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>0.336955</td>\n",
       "      <td>-0.671871</td>\n",
       "      <td>0.334916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>0.334553</td>\n",
       "      <td>0.338540</td>\n",
       "      <td>-0.673093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>-0.660152</td>\n",
       "      <td>0.364550</td>\n",
       "      <td>0.295602</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2\n",
       "24  -0.676437  0.326065  0.350373\n",
       "278  0.367151  0.361796 -0.728947\n",
       "93  -0.617728  0.399298  0.218430\n",
       "159  0.296441 -0.723896  0.427454\n",
       "18  -0.678117  0.319030  0.359087\n",
       "5   -0.669465  0.328544  0.340921\n",
       "235  0.352386  0.366542 -0.718928\n",
       "286  0.347930  0.324548 -0.672478\n",
       "299  0.293485  0.257899 -0.551384\n",
       "181  0.276691 -0.736109  0.459417\n",
       "273  0.355830  0.342016 -0.697846\n",
       "160  0.295189 -0.725142  0.429953\n",
       "52  -0.670353  0.346545  0.323808\n",
       "223  0.341604  0.352069 -0.693673\n",
       "238  0.354285  0.369268 -0.723553\n",
       "61  -0.657277  0.369434  0.287843\n",
       "86  -0.620876  0.404349  0.216527\n",
       "156  0.295567 -0.721834  0.426268\n",
       "100  0.333333 -0.666667  0.333333\n",
       "208  0.333274  0.336068 -0.669342\n",
       "162  0.289533 -0.726322  0.436789\n",
       "168  0.286244 -0.725192  0.438948\n",
       "289  0.311371  0.278660 -0.590031\n",
       "193  0.285675 -0.712078  0.426404\n",
       "271  0.371739  0.381675 -0.753414\n",
       "233  0.349993  0.364316 -0.714310\n",
       "132  0.335113 -0.675201  0.340087\n",
       "115  0.337782 -0.665087  0.327305\n",
       "255  0.363460  0.369751 -0.733210\n",
       "75  -0.638994  0.397917  0.241078\n",
       "..        ...       ...       ...\n",
       "190  0.281044 -0.721658  0.440614\n",
       "6   -0.670536  0.327712  0.342825\n",
       "218  0.341168  0.349230 -0.690398\n",
       "30  -0.681543  0.319292  0.362251\n",
       "261  0.366428  0.373407 -0.739835\n",
       "149  0.312488 -0.706968  0.394480\n",
       "117  0.339394 -0.663105  0.323711\n",
       "154  0.302641 -0.717316  0.414675\n",
       "37  -0.675219  0.333249  0.341970\n",
       "151  0.300231 -0.716758  0.416527\n",
       "60  -0.660779  0.364042  0.296737\n",
       "173  0.281085 -0.734549  0.453464\n",
       "54  -0.664794  0.355792  0.309002\n",
       "291  0.329059  0.298012 -0.627072\n",
       "124  0.339573 -0.665631  0.326058\n",
       "293  0.331291  0.300113 -0.631405\n",
       "143  0.315821 -0.701497  0.385676\n",
       "8   -0.671739  0.325630  0.346109\n",
       "128  0.332901 -0.676883  0.343982\n",
       "125  0.337582 -0.669156  0.331574\n",
       "230  0.342472  0.356145 -0.698618\n",
       "283  0.324742  0.295321 -0.620063\n",
       "226  0.343298  0.355195 -0.698493\n",
       "36  -0.675738  0.332017  0.343721\n",
       "174  0.280193 -0.736334  0.456141\n",
       "281  0.335619  0.309321 -0.644940\n",
       "263  0.365148  0.367154 -0.732302\n",
       "130  0.336955 -0.671871  0.334916\n",
       "210  0.334553  0.338540 -0.673093\n",
       "59  -0.660152  0.364550  0.295602\n",
       "\n",
       "[300 rows x 3 columns]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gradient of loss wrt pre activation Wx + b.\n",
    "grad_loss = np.subtract(p, Y)\n",
    "grad_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.0457932 , -0.0532089 ,  0.10069507],\n",
       "       [ 0.07510256, -0.11419095,  0.04004541]])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gradient for weights = XT * (1/N)grad_loss + learn_rate * W\n",
    "XT = np.transpose(X)\n",
    "learn_rate = 0.001\n",
    "\n",
    "W_grad = np.dot(XT, np.divide(grad_loss, N)) + np.multiply(W, learn_rate)\n",
    "\n",
    "W_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.multiply(Y, np.log(p)).shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.2078232 , -0.3550293 ],\n",
       "       [-0.96237392, -0.48124256],\n",
       "       [-0.89614661, -0.52448407],\n",
       "       [-0.69314718, -0.69314718]])"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
